{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "324553e2",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf866e12",
   "metadata": {},
   "source": [
    "Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ad1abe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2828c2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ceec1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check for CUDA\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a426aca2",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689ca7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from dataset import TransliterationDataset, collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4009cc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(dataset_path, batch_size=64, shuffle=True):\n",
    "    dataset = TransliterationDataset(dataset_path)\n",
    "    data_loader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=shuffle, \n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    return dataset, data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "673fca47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\m'\n",
      "C:\\Users\\Ayaan\\AppData\\Local\\Temp\\ipykernel_19472\\2386480759.py:2: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  train_path = 'dakshina_dataset_v1.0\\ml\\lexicons\\ml.translit.sampled.train.tsv'\n",
      "C:\\Users\\Ayaan\\AppData\\Local\\Temp\\ipykernel_19472\\2386480759.py:3: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  val_path = 'dakshina_dataset_v1.0\\ml\\lexicons\\ml.translit.sampled.dev.tsv'\n",
      "C:\\Users\\Ayaan\\AppData\\Local\\Temp\\ipykernel_19472\\2386480759.py:4: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  test_path = 'dakshina_dataset_v1.0\\ml\\lexicons\\ml.translit.sampled.test.tsv'\n"
     ]
    }
   ],
   "source": [
    "# Dataset paths\n",
    "train_path = 'dakshina_dataset_v1.0\\ml\\lexicons\\ml.translit.sampled.train.tsv'\n",
    "val_path = 'dakshina_dataset_v1.0\\ml\\lexicons\\ml.translit.sampled.dev.tsv'\n",
    "test_path = 'dakshina_dataset_v1.0\\ml\\lexicons\\ml.translit.sampled.test.tsv'\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataset, train_loader = prepare_data(train_path, batch_size=64)\n",
    "val_dataset, val_loader = prepare_data(val_path, batch_size=64)\n",
    "test_dataset, test_loader = prepare_data(test_path, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "204978fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source vocabulary size: 29\n",
      "Target vocabulary size: 73\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "input_size = train_dataset.get_vocab_size('source')\n",
    "output_size = train_dataset.get_vocab_size('target')\n",
    "\n",
    "# Print vocabulary sizes\n",
    "print(f\"Source vocabulary size: {input_size}\")\n",
    "print(f\"Target vocabulary size: {output_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8455d86a",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e881045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "\n",
    "from models import Encoder, Decoder, Seq2Seq\n",
    "from training import train, evaluate, transliterate, calculate_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f57ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(input_size, output_size, train_loader, val_loader, device, embedding_size=256, hidden_size=512, \n",
    "                n_layers=1, dropout=0.1, cell_type='lstm', epochs=10, teacher_forcing_ratio=0.5, clip=1.0):  \n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create model\n",
    "    encoder = Encoder(\n",
    "        input_size=input_size,\n",
    "        embedding_size=embedding_size,\n",
    "        hidden_size=hidden_size,\n",
    "        n_layers=n_layers,\n",
    "        dropout=dropout,\n",
    "        cell_type=cell_type\n",
    "    ).to(device)\n",
    "    \n",
    "    decoder = Decoder(\n",
    "        output_size=output_size,\n",
    "        embedding_size=embedding_size,\n",
    "        hidden_size=hidden_size,\n",
    "        n_layers=n_layers,\n",
    "        dropout=dropout,\n",
    "        cell_type=cell_type\n",
    "    ).to(device)\n",
    "    \n",
    "    model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "    \n",
    "    # Define optimizer and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding index (0)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    # Lists to store losses\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Starting training...\")\n",
    "    pbar = trange(epochs, desc=\"Epoch\", dynamic_ncols=True)\n",
    "    for epoch in pbar:\n",
    "        # Train\n",
    "        train_loss = train(\n",
    "            model=model,\n",
    "            device=device,\n",
    "            dataloader=train_loader,\n",
    "            optimizer=optimizer,\n",
    "            criterion=criterion,\n",
    "            clip=clip,\n",
    "            teacher_forcing_ratio=teacher_forcing_ratio\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss = evaluate(\n",
    "            model=model,\n",
    "            device=device,\n",
    "            dataloader=val_loader,\n",
    "            criterion=criterion\n",
    "        )\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Update tqdm bar description\n",
    "        pbar.set_description(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        pbar.set_postfix(train_loss=f\"{train_loss:.4f}\", val_loss=f\"{val_loss:.4f}\")\n",
    "            \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(\"Best model saved!\")\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56e4ec39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:   5%|▌         | 1/20 [00:48<15:18, 48.34s/it, train_loss=1.2254, val_loss=9.5475]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20:  20%|██        | 4/20 [03:16<13:05, 49.09s/it, train_loss=0.2214, val_loss=12.8525]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m train_model(input_size\u001b[38;5;241m=\u001b[39minput_size, output_size\u001b[38;5;241m=\u001b[39moutput_size, train_loader\u001b[38;5;241m=\u001b[39mtrain_loader, \n\u001b[0;32m      2\u001b[0m                                        val_loader\u001b[38;5;241m=\u001b[39mval_loader, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "Cell \u001b[1;32mIn[18], line 41\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(input_size, output_size, train_loader, val_loader, device, embedding_size, hidden_size, n_layers, dropout, cell_type, epochs, teacher_forcing_ratio, clip)\u001b[0m\n\u001b[0;32m     38\u001b[0m pbar \u001b[38;5;241m=\u001b[39m trange(epochs, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m\"\u001b[39m, dynamic_ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train(\n\u001b[0;32m     42\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     43\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m     44\u001b[0m         dataloader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[0;32m     45\u001b[0m         optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m     46\u001b[0m         criterion\u001b[38;5;241m=\u001b[39mcriterion,\n\u001b[0;32m     47\u001b[0m         clip\u001b[38;5;241m=\u001b[39mclip,\n\u001b[0;32m     48\u001b[0m         teacher_forcing_ratio\u001b[38;5;241m=\u001b[39mteacher_forcing_ratio\n\u001b[0;32m     49\u001b[0m     )\n\u001b[0;32m     50\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m# Validate\u001b[39;00m\n",
      "File \u001b[1;32md:\\Ayaan\\IITM\\Courses\\Sem 8\\DA6401\\DA6401-Assignment-3\\training.py:45\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, device, dataloader, optimizer, criterion, clip, teacher_forcing_ratio)\u001b[0m\n\u001b[0;32m     42\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Clip gradients\u001b[39;00m\n\u001b[0;32m     48\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), clip)\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    628\u001b[0m )\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = train_model(input_size=input_size, output_size=output_size, train_loader=train_loader, \n",
    "                                       val_loader=val_loader, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d5d824",
   "metadata": {},
   "source": [
    "### Visualize training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce115696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7fe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53966fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.savefig('loss_plot.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6924fb4",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning with Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ba7ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cba31c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with wandb integration\n",
    "def train_with_wandb(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        # Access wandb config\n",
    "        config = wandb.config\n",
    "        \n",
    "        # Load datasets\n",
    "        train_dataset = TransliterationDataset('dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv')\n",
    "        val_dataset = TransliterationDataset('dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv')\n",
    "        \n",
    "        # Make sure we're using the same vocabulary across datasets\n",
    "        val_dataset.source_char_to_idx = train_dataset.source_char_to_idx\n",
    "        val_dataset.source_idx_to_char = train_dataset.source_idx_to_char\n",
    "        val_dataset.target_char_to_idx = train_dataset.target_char_to_idx\n",
    "        val_dataset.target_idx_to_char = train_dataset.target_idx_to_char\n",
    "        \n",
    "        # Create dataloaders\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "        \n",
    "        # Initialize model components\n",
    "        input_size = train_dataset.get_vocab_size('source')\n",
    "        output_size = train_dataset.get_vocab_size('target')\n",
    "        \n",
    "        encoder = Encoder(\n",
    "            input_size=input_size, \n",
    "            embedding_size=config.embedding_size, \n",
    "            hidden_size=config.hidden_size, \n",
    "            n_layers=config.encoder_layers, \n",
    "            dropout=config.dropout,\n",
    "            cell_type=config.cell_type\n",
    "        )\n",
    "        \n",
    "        decoder = Decoder(\n",
    "            output_size=output_size, \n",
    "            embedding_size=config.embedding_size, \n",
    "            hidden_size=config.hidden_size, \n",
    "            n_layers=config.decoder_layers, \n",
    "            dropout=config.dropout,\n",
    "            cell_type=config.cell_type\n",
    "        )\n",
    "        \n",
    "        model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "        \n",
    "        # Set up optimizer and loss function\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.target_char_to_idx['<PAD>'])\n",
    "        \n",
    "        # Track best validation loss\n",
    "        best_valid_loss = float('inf')\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(config.epochs):\n",
    "            # Train\n",
    "            train_loss = train(model, train_dataloader, optimizer, criterion, \n",
    "                             clip=config.clip, teacher_forcing_ratio=config.teacher_forcing_ratio)\n",
    "            \n",
    "            # Evaluate\n",
    "            val_loss = evaluate(model, val_dataloader, criterion)\n",
    "            \n",
    "            # Log metrics\n",
    "            wandb.log({\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'epoch': epoch\n",
    "            })\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < best_valid_loss:\n",
    "                best_valid_loss = val_loss\n",
    "                torch.save(model.state_dict(), f'model_{config.cell_type}.pt')\n",
    "                print(f'Model saved at epoch {epoch+1}')\n",
    "            \n",
    "            print(f'Epoch: {epoch+1}')\n",
    "            print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "            print(f'\\tVal Loss: {val_loss:.3f}')\n",
    "        \n",
    "        # Evaluation on validation set\n",
    "        val_accuracy, val_predictions = calculate_accuracy(model, val_dataloader, train_dataset)\n",
    "        \n",
    "        # Log final metrics\n",
    "        wandb.log({\n",
    "            'val_accuracy': val_accuracy\n",
    "        })\n",
    "        \n",
    "        print(f'Validation Accuracy: {val_accuracy:.3f}')\n",
    "        \n",
    "        # Sample predictions\n",
    "        for i, (source, pred, target) in enumerate(val_predictions[:10]):\n",
    "            print(f'Source: {source}')\n",
    "            print(f'Prediction: {pred}')\n",
    "            print(f'Target: {target}')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8834fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sweep configuration\n",
    "sweep_config = {\n",
    "    'method': 'grid',  # We can also use 'random' or 'bayes' for more efficient searching\n",
    "    'metric': {\n",
    "        'name': 'val_loss',\n",
    "        'goal': 'minimize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'learning_rate': {\n",
    "            'values': [0.001, 0.0001]\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [32, 64]\n",
    "        },\n",
    "        'embedding_size': {\n",
    "            'values': [64, 128, 256]\n",
    "        },\n",
    "        'hidden_size': {\n",
    "            'values': [128, 256]\n",
    "        },\n",
    "        'encoder_layers': {\n",
    "            'values': [1, 2]\n",
    "        },\n",
    "        'decoder_layers': {\n",
    "            'values': [1, 2]\n",
    "        },\n",
    "        'dropout': {\n",
    "            'values': [0.1, 0.3]\n",
    "        },\n",
    "        'cell_type': {\n",
    "            'values': ['rnn', 'lstm', 'gru']\n",
    "        },\n",
    "        'teacher_forcing_ratio': {\n",
    "            'values': [0.5, 0.7]\n",
    "        },\n",
    "        'clip': {\n",
    "            'values': [1.0]\n",
    "        },\n",
    "        'epochs': {\n",
    "            'values': [10]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01797b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sweep():\n",
    "    # Initialize wandb\n",
    "    sweep_id = wandb.sweep(sweep_config, project='DA6401-Assignment-3')\n",
    "    \n",
    "    # Run sweep\n",
    "    wandb.agent(sweep_id, train_model, count=24)  # You can adjust the count based on resources\n",
    "    \n",
    "    # Find best model configuration\n",
    "    api = wandb.Api()\n",
    "    sweep = api.sweep(f\"your_username/DA6401-Assignment-3/{sweep_id}\")\n",
    "    best_run = sweep.best_run()\n",
    "    best_config = best_run.config\n",
    "    \n",
    "    print(\"Best Configuration:\")\n",
    "    print(best_config)\n",
    "    \n",
    "    return best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc2782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run hyperparameter sweep\n",
    "best_config = run_sweep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e47cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_errors(predictions):\n",
    "    \"\"\"\n",
    "    Analyze the errors made by the model\n",
    "    \"\"\"\n",
    "    # Count total predictions and correct predictions\n",
    "    total = len(predictions)\n",
    "    correct = sum(1 for _, pred, target in predictions if pred == target)\n",
    "    \n",
    "    print(f'Accuracy: {correct/total:.3f} ({correct}/{total})')\n",
    "    \n",
    "    # Analyze error patterns\n",
    "    errors = [(source, pred, target) for source, pred, target in predictions if pred != target]\n",
    "    \n",
    "    # Error by length\n",
    "    length_errors = {}\n",
    "    for source, _, target in errors:\n",
    "        length = len(source)\n",
    "        if length not in length_errors:\n",
    "            length_errors[length] = 0\n",
    "        length_errors[length] += 1\n",
    "    \n",
    "    # Sort by length\n",
    "    sorted_length_errors = {k: v for k, v in sorted(length_errors.items())}\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(sorted_length_errors.keys(), sorted_length_errors.values())\n",
    "    plt.xlabel('Source Length')\n",
    "    plt.ylabel('Number of Errors')\n",
    "    plt.title('Errors by Source Length')\n",
    "    plt.savefig('predictions_vanilla/errors_by_length.png')\n",
    "    \n",
    "    # Sample error analysis\n",
    "    print(\"\\nSample Error Analysis:\")\n",
    "    for i, (source, pred, target) in enumerate(errors[:10]):\n",
    "        print(f'Source: {source}')\n",
    "        print(f'Prediction: {pred}')\n",
    "        print(f'Target: {target}')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e2f6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_best_model(best_config):\n",
    "    # Load datasets\n",
    "    train_dataset = TransliterationDataset('dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv')\n",
    "    test_dataset = TransliterationDataset('dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv')\n",
    "    \n",
    "    # Make sure test dataset uses the same vocabulary as training\n",
    "    test_dataset.source_char_to_idx = train_dataset.source_char_to_idx\n",
    "    test_dataset.source_idx_to_char = train_dataset.source_idx_to_char\n",
    "    test_dataset.target_char_to_idx = train_dataset.target_char_to_idx\n",
    "    test_dataset.target_idx_to_char = train_dataset.target_idx_to_char\n",
    "    \n",
    "    # Create dataloader\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    # Initialize model with best configuration\n",
    "    input_size = train_dataset.get_vocab_size('source')\n",
    "    output_size = train_dataset.get_vocab_size('target')\n",
    "    \n",
    "    best_encoder = Encoder(\n",
    "        input_size=input_size, \n",
    "        embedding_size=best_config['embedding_size'], \n",
    "        hidden_size=best_config['hidden_size'], \n",
    "        n_layers=best_config['encoder_layers'], \n",
    "        dropout=best_config['dropout'],\n",
    "        cell_type=best_config['cell_type']\n",
    "    )\n",
    "    \n",
    "    best_decoder = Decoder(\n",
    "        output_size=output_size, \n",
    "        embedding_size=best_config['embedding_size'], \n",
    "        hidden_size=best_config['hidden_size'], \n",
    "        n_layers=best_config['decoder_layers'], \n",
    "        dropout=best_config['dropout'],\n",
    "        cell_type=best_config['cell_type']\n",
    "    )\n",
    "    \n",
    "    best_model = Seq2Seq(best_encoder, best_decoder, device).to(device)\n",
    "    \n",
    "    # Load model parameters\n",
    "    best_model.load_state_dict(torch.load(f\"model_{best_config['cell_type']}.pt\"))\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_accuracy, test_predictions = calculate_accuracy(best_model, test_dataloader, train_dataset)\n",
    "    \n",
    "    print(f'Test Accuracy: {test_accuracy:.3f}')\n",
    "    \n",
    "    # Save predictions to file\n",
    "    with open('predictions_vanilla/test_predictions.txt', 'w', encoding='utf-8') as f:\n",
    "        for source, pred, target in test_predictions:\n",
    "            f.write(f'Source: {source}\\n')\n",
    "            f.write(f'Prediction: {pred}\\n')\n",
    "            f.write(f'Target: {target}\\n\\n')\n",
    "    \n",
    "    # Create error analysis\n",
    "    analyze_errors(test_predictions)\n",
    "    \n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27edbd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure predictions directory exists\n",
    "os.makedirs('predictions_vanilla', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbacaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test best model\n",
    "test_predictions = test_best_model(best_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
